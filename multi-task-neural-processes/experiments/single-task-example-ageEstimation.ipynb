{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Process for age estimation stochastic process\n",
    "\n",
    "This notebook shows how to train and sample from a Neural Process for age estimation\n",
    "\n",
    "We select the FG-net, each person as a batch. In each batch, there are 18 persons.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from datasets import FaceFeatureData\n",
    "\n",
    "# Create dataset\n",
    "num_of_people = 3\n",
    "num_of_images=18\n",
    "dataset = FaceFeatureData(num_of_people=num_of_people,num_of_images=num_of_images)\n",
    "\n",
    "#82 different people(batch_num)\n",
    "#18 different images each people(batch_size)\n",
    "#x_dim = 2048\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build Neural Process"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from neural_process import NeuralProcess\n",
    "\n",
    "x_dim = 2048\n",
    "y_dim = 1\n",
    "r_dim = 50  # Dimension of representation of context points\n",
    "z_dim = 50  # Dimension of sampled latent variable\n",
    "h_dim = 50  # Dimension of hidden layers in encoder and decoder\n",
    "\n",
    "neuralprocess = NeuralProcess(x_dim, y_dim, r_dim, z_dim, h_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Neural Process\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch: 0, Avg_loss: 10846.663513183594\n",
      "Epoch: 1, Avg_loss: 98.95793151855469\n",
      "Epoch: 2, Avg_loss: 98.63635762532552\n",
      "Epoch: 3, Avg_loss: 98.37577311197917\n",
      "Epoch: 4, Avg_loss: 97.50936126708984\n",
      "Epoch: 5, Avg_loss: 96.99312082926433\n",
      "Epoch: 6, Avg_loss: 97.3414815266927\n",
      "Epoch: 7, Avg_loss: 96.20968119303386\n",
      "Epoch: 8, Avg_loss: 95.0952631632487\n",
      "Epoch: 9, Avg_loss: 94.89576466878255\n",
      "Epoch: 10, Avg_loss: 94.31024932861328\n",
      "Epoch: 11, Avg_loss: 94.21786244710286\n",
      "Epoch: 12, Avg_loss: 93.51469167073567\n",
      "Epoch: 13, Avg_loss: 93.19683074951172\n",
      "Epoch: 14, Avg_loss: 92.5988998413086\n",
      "Epoch: 15, Avg_loss: 92.0246810913086\n",
      "Epoch: 16, Avg_loss: 91.26009368896484\n",
      "Epoch: 17, Avg_loss: 91.20225016276042\n",
      "Epoch: 18, Avg_loss: 90.40600077311198\n",
      "Epoch: 19, Avg_loss: 90.17312622070312\n",
      "Epoch: 20, Avg_loss: 90.04954528808594\n",
      "Epoch: 21, Avg_loss: 89.39864095052083\n",
      "Epoch: 22, Avg_loss: 88.80863698323567\n",
      "Epoch: 23, Avg_loss: 88.60177866617839\n",
      "Epoch: 24, Avg_loss: 88.12396494547527\n",
      "Epoch: 25, Avg_loss: 87.63652038574219\n",
      "Epoch: 26, Avg_loss: 87.06519571940105\n",
      "Epoch: 27, Avg_loss: 86.60509490966797\n",
      "Epoch: 28, Avg_loss: 86.2622578938802\n",
      "Epoch: 29, Avg_loss: 86.02651468912761\n",
      "Epoch: 30, Avg_loss: 85.8597920735677\n",
      "Epoch: 31, Avg_loss: 85.45116170247395\n",
      "Epoch: 32, Avg_loss: 85.14722188313802\n",
      "Epoch: 33, Avg_loss: 84.87196350097656\n",
      "Epoch: 34, Avg_loss: 84.75443013509114\n",
      "Epoch: 35, Avg_loss: 84.57074737548828\n",
      "Epoch: 36, Avg_loss: 84.34170277913411\n",
      "Epoch: 37, Avg_loss: 84.28210957845052\n",
      "Epoch: 38, Avg_loss: 83.91373189290364\n",
      "Epoch: 39, Avg_loss: 83.88955688476562\n",
      "Epoch: 40, Avg_loss: 83.69400787353516\n",
      "Epoch: 41, Avg_loss: 83.49395497639973\n",
      "Epoch: 42, Avg_loss: 83.41327158610027\n",
      "Epoch: 43, Avg_loss: 83.3208745320638\n",
      "Epoch: 44, Avg_loss: 83.24808502197266\n",
      "Epoch: 45, Avg_loss: 83.18170674641927\n",
      "Epoch: 46, Avg_loss: 83.11531575520833\n",
      "Epoch: 47, Avg_loss: 82.87210083007812\n",
      "Epoch: 48, Avg_loss: 82.94313049316406\n",
      "Epoch: 49, Avg_loss: 82.84505716959636\n",
      "Epoch: 50, Avg_loss: 82.62400817871094\n",
      "Epoch: 51, Avg_loss: 82.56117248535156\n",
      "Epoch: 52, Avg_loss: 82.57494099934895\n",
      "Epoch: 53, Avg_loss: 82.46388244628906\n",
      "Epoch: 54, Avg_loss: 82.47489166259766\n",
      "Epoch: 55, Avg_loss: 82.41436767578125\n",
      "Epoch: 56, Avg_loss: 82.2156982421875\n",
      "Epoch: 57, Avg_loss: 82.19924672444661\n",
      "Epoch: 58, Avg_loss: 82.11458333333333\n",
      "Epoch: 59, Avg_loss: 82.2445068359375\n",
      "Epoch: 60, Avg_loss: 81.95650990804036\n",
      "Epoch: 61, Avg_loss: 81.84737396240234\n",
      "Epoch: 62, Avg_loss: 81.71657816569011\n",
      "Epoch: 63, Avg_loss: 81.63717905680339\n",
      "Epoch: 64, Avg_loss: 81.62401835123698\n",
      "Epoch: 65, Avg_loss: 81.54783376057942\n",
      "Epoch: 66, Avg_loss: 81.41329701741536\n",
      "Epoch: 67, Avg_loss: 81.30468241373698\n",
      "Epoch: 68, Avg_loss: 81.32489522298177\n",
      "Epoch: 69, Avg_loss: 81.16042836507161\n",
      "Epoch: 70, Avg_loss: 81.10472869873047\n",
      "Epoch: 71, Avg_loss: 81.04899342854817\n",
      "Epoch: 72, Avg_loss: 80.78925069173177\n",
      "Epoch: 73, Avg_loss: 80.75609079996745\n",
      "Epoch: 74, Avg_loss: 80.71942138671875\n",
      "Epoch: 75, Avg_loss: 80.5185038248698\n",
      "Epoch: 76, Avg_loss: 80.48015594482422\n",
      "Epoch: 77, Avg_loss: 80.45572153727214\n",
      "Epoch: 78, Avg_loss: 80.20400492350261\n",
      "Epoch: 79, Avg_loss: 80.12241109212239\n",
      "Epoch: 80, Avg_loss: 80.00704447428386\n",
      "Epoch: 81, Avg_loss: 79.71214803059895\n",
      "Epoch: 82, Avg_loss: 79.79640706380208\n",
      "Epoch: 83, Avg_loss: 79.73897043863933\n",
      "Epoch: 84, Avg_loss: 79.56442515055339\n",
      "Epoch: 85, Avg_loss: 79.39668273925781\n",
      "Epoch: 86, Avg_loss: 79.08922322591145\n",
      "Epoch: 87, Avg_loss: 79.15723164876302\n",
      "Epoch: 88, Avg_loss: 78.89958953857422\n",
      "Epoch: 89, Avg_loss: 78.76585642496745\n",
      "Epoch: 90, Avg_loss: 78.57746887207031\n",
      "Epoch: 91, Avg_loss: 78.38645680745442\n",
      "Epoch: 92, Avg_loss: 78.2366714477539\n",
      "Epoch: 93, Avg_loss: 77.95468139648438\n",
      "Epoch: 94, Avg_loss: 77.78490447998047\n",
      "Epoch: 95, Avg_loss: 77.75497690836589\n",
      "Epoch: 96, Avg_loss: 77.70401509602864\n",
      "Epoch: 97, Avg_loss: 77.11569468180339\n",
      "Epoch: 98, Avg_loss: 76.76828002929688\n",
      "Epoch: 99, Avg_loss: 76.80118052164714\n",
      "Epoch: 100, Avg_loss: 76.78638203938802\n",
      "Epoch: 101, Avg_loss: 76.596923828125\n",
      "Epoch: 102, Avg_loss: 76.18184153238933\n",
      "Epoch: 103, Avg_loss: 75.63441467285156\n",
      "Epoch: 104, Avg_loss: 75.39350128173828\n",
      "Epoch: 105, Avg_loss: 75.25111897786458\n",
      "Epoch: 106, Avg_loss: 74.81237030029297\n",
      "Epoch: 107, Avg_loss: 74.59820810953777\n",
      "Epoch: 108, Avg_loss: 74.43506113688152\n",
      "Epoch: 109, Avg_loss: 73.77169799804688\n",
      "Epoch: 110, Avg_loss: 73.2217534383138\n",
      "Epoch: 111, Avg_loss: 73.06479136149089\n",
      "Epoch: 112, Avg_loss: 72.46820068359375\n",
      "Epoch: 113, Avg_loss: 71.82680765787761\n",
      "Epoch: 114, Avg_loss: 72.1280008951823\n",
      "Epoch: 115, Avg_loss: 71.17531077067058\n",
      "Epoch: 116, Avg_loss: 70.82244110107422\n",
      "Epoch: 117, Avg_loss: 69.8982925415039\n",
      "Epoch: 118, Avg_loss: 69.71299997965495\n",
      "Epoch: 119, Avg_loss: 69.03170267740886\n",
      "Epoch: 120, Avg_loss: 69.02262878417969\n",
      "Epoch: 121, Avg_loss: 68.91883341471355\n",
      "Epoch: 122, Avg_loss: 68.167236328125\n",
      "Epoch: 123, Avg_loss: 68.3203837076823\n",
      "Epoch: 124, Avg_loss: 68.06438700358073\n",
      "Epoch: 125, Avg_loss: 68.24740600585938\n",
      "Epoch: 126, Avg_loss: 68.15661875406902\n",
      "Epoch: 127, Avg_loss: 68.14432271321614\n",
      "Epoch: 128, Avg_loss: 68.14344278971355\n",
      "Epoch: 129, Avg_loss: 68.22494252522786\n",
      "Epoch: 130, Avg_loss: 67.97024536132812\n",
      "Epoch: 131, Avg_loss: 67.66705322265625\n",
      "Epoch: 132, Avg_loss: 67.59919738769531\n",
      "Epoch: 133, Avg_loss: 67.13267771402995\n",
      "Epoch: 134, Avg_loss: 67.6507937113444\n",
      "Epoch: 135, Avg_loss: 68.24674987792969\n",
      "Epoch: 136, Avg_loss: 67.70484161376953\n",
      "Epoch: 137, Avg_loss: 67.73616790771484\n",
      "Epoch: 138, Avg_loss: 68.30068969726562\n",
      "Epoch: 139, Avg_loss: 68.16220092773438\n",
      "Epoch: 140, Avg_loss: 67.41289520263672\n",
      "Epoch: 141, Avg_loss: 67.65252431233723\n",
      "Epoch: 142, Avg_loss: 67.22249603271484\n",
      "Epoch: 143, Avg_loss: 66.693421681722\n",
      "Epoch: 144, Avg_loss: 67.71185557047527\n",
      "Epoch: 145, Avg_loss: 68.21399943033855\n",
      "Epoch: 146, Avg_loss: 67.6615727742513\n",
      "Epoch: 147, Avg_loss: 68.26737976074219\n",
      "Epoch: 148, Avg_loss: 68.31774648030598\n",
      "Epoch: 149, Avg_loss: 67.07616170247395\n",
      "Epoch: 150, Avg_loss: 67.59039815266927\n",
      "Epoch: 151, Avg_loss: 67.9477310180664\n",
      "Epoch: 152, Avg_loss: 66.89072926839192\n",
      "Epoch: 153, Avg_loss: 67.5235087076823\n",
      "Epoch: 154, Avg_loss: 68.0833002726237\n",
      "Epoch: 155, Avg_loss: 67.54848988850911\n",
      "Epoch: 156, Avg_loss: 67.52662658691406\n",
      "Epoch: 157, Avg_loss: 67.7829106648763\n",
      "Epoch: 158, Avg_loss: 67.6793441772461\n",
      "Epoch: 159, Avg_loss: 68.55780792236328\n",
      "Epoch: 160, Avg_loss: 67.74154154459636\n",
      "Epoch: 161, Avg_loss: 67.29899470011394\n",
      "Epoch: 162, Avg_loss: 67.0517069498698\n",
      "Epoch: 163, Avg_loss: 68.02507781982422\n",
      "Epoch: 164, Avg_loss: 66.9976298014323\n",
      "Epoch: 165, Avg_loss: 67.79604848225911\n",
      "Epoch: 166, Avg_loss: 67.62983703613281\n",
      "Epoch: 167, Avg_loss: 67.9999287923177\n",
      "Epoch: 168, Avg_loss: 67.9180196126302\n",
      "Epoch: 169, Avg_loss: 67.64500681559245\n",
      "Epoch: 170, Avg_loss: 68.53722127278645\n",
      "Epoch: 171, Avg_loss: 67.35530344645183\n",
      "Epoch: 172, Avg_loss: 67.60065078735352\n",
      "Epoch: 173, Avg_loss: 68.25655364990234\n",
      "Epoch: 174, Avg_loss: 67.83814493815105\n",
      "Epoch: 175, Avg_loss: 67.31753794352214\n",
      "Epoch: 176, Avg_loss: 67.4337895711263\n",
      "Epoch: 177, Avg_loss: 67.9973627726237\n",
      "Epoch: 178, Avg_loss: 67.28042602539062\n",
      "Epoch: 179, Avg_loss: 66.997865041097\n",
      "Epoch: 180, Avg_loss: 66.89853159586589\n",
      "Epoch: 181, Avg_loss: 68.23313395182292\n",
      "Epoch: 182, Avg_loss: 67.41162999471028\n",
      "Epoch: 183, Avg_loss: 67.6013692220052\n",
      "Epoch: 184, Avg_loss: 67.4773178100586\n",
      "Epoch: 185, Avg_loss: 66.71612040201823\n",
      "Epoch: 186, Avg_loss: 67.45807139078777\n",
      "Epoch: 187, Avg_loss: 67.20219039916992\n",
      "Epoch: 188, Avg_loss: 68.2793795267741\n",
      "Epoch: 189, Avg_loss: 66.8120600382487\n",
      "Epoch: 190, Avg_loss: 67.17087046305339\n",
      "Epoch: 191, Avg_loss: 67.51632563273112\n",
      "Epoch: 192, Avg_loss: 66.72993342081706\n",
      "Epoch: 193, Avg_loss: 67.67531458536784\n",
      "Epoch: 194, Avg_loss: 66.93038558959961\n",
      "Epoch: 195, Avg_loss: 67.5799051920573\n",
      "Epoch: 196, Avg_loss: 67.2264633178711\n",
      "Epoch: 197, Avg_loss: 67.38900629679362\n",
      "Epoch: 198, Avg_loss: 66.55072530110677\n",
      "Epoch: 199, Avg_loss: 67.13355763753255\n",
      "Epoch: 200, Avg_loss: 67.2078030904134\n",
      "Epoch: 201, Avg_loss: 67.08621215820312\n",
      "Epoch: 202, Avg_loss: 66.94658533732097\n",
      "Epoch: 203, Avg_loss: 67.25582631429036\n",
      "Epoch: 204, Avg_loss: 67.01826095581055\n",
      "Epoch: 205, Avg_loss: 66.64872614542644\n",
      "Epoch: 206, Avg_loss: 67.24305597941081\n",
      "Epoch: 207, Avg_loss: 67.58190536499023\n",
      "Epoch: 208, Avg_loss: 67.76334635416667\n",
      "Epoch: 209, Avg_loss: 67.28351720174153\n",
      "Epoch: 210, Avg_loss: 67.32312520345052\n",
      "Epoch: 211, Avg_loss: 66.965576171875\n",
      "Epoch: 212, Avg_loss: 66.78162384033203\n",
      "Epoch: 213, Avg_loss: 66.80813852945964\n",
      "Epoch: 214, Avg_loss: 67.21472295125325\n",
      "Epoch: 215, Avg_loss: 66.11931228637695\n",
      "Epoch: 216, Avg_loss: 66.72403081258138\n",
      "Epoch: 217, Avg_loss: 67.50005467732747\n",
      "Epoch: 218, Avg_loss: 66.77828470865886\n",
      "Epoch: 219, Avg_loss: 66.6796480814616\n",
      "Epoch: 220, Avg_loss: 67.21662267049153\n",
      "Epoch: 221, Avg_loss: 67.26611836751302\n",
      "Epoch: 222, Avg_loss: 66.7826156616211\n",
      "Epoch: 223, Avg_loss: 67.04780832926433\n",
      "Epoch: 224, Avg_loss: 66.47205098470052\n",
      "Epoch: 225, Avg_loss: 66.81278610229492\n",
      "Epoch: 226, Avg_loss: 67.19329579671223\n",
      "Epoch: 227, Avg_loss: 67.79490661621094\n",
      "Epoch: 228, Avg_loss: 66.66394805908203\n",
      "Epoch: 229, Avg_loss: 67.18583424886067\n",
      "Epoch: 230, Avg_loss: 66.9962552388509\n",
      "Epoch: 231, Avg_loss: 67.14450581868489\n",
      "Epoch: 232, Avg_loss: 66.8626314798991\n",
      "Epoch: 233, Avg_loss: 66.75328572591145\n",
      "Epoch: 234, Avg_loss: 66.23418553670247\n",
      "Epoch: 235, Avg_loss: 66.73151397705078\n",
      "Epoch: 236, Avg_loss: 66.21218617757161\n",
      "Epoch: 237, Avg_loss: 67.07131830851237\n",
      "Epoch: 238, Avg_loss: 66.84471130371094\n",
      "Epoch: 239, Avg_loss: 67.5601806640625\n",
      "Epoch: 240, Avg_loss: 67.45536041259766\n",
      "Epoch: 241, Avg_loss: 67.47212346394856\n",
      "Epoch: 242, Avg_loss: 67.24980290730794\n",
      "Epoch: 243, Avg_loss: 66.59584299723308\n",
      "Epoch: 244, Avg_loss: 66.9275271097819\n",
      "Epoch: 245, Avg_loss: 66.48295211791992\n",
      "Epoch: 246, Avg_loss: 67.1442985534668\n",
      "Epoch: 247, Avg_loss: 66.90059026082356\n",
      "Epoch: 248, Avg_loss: 66.80253982543945\n",
      "Epoch: 249, Avg_loss: 66.24090830485027\n",
      "Epoch: 250, Avg_loss: 67.2762222290039\n",
      "Epoch: 251, Avg_loss: 67.009521484375\n",
      "Epoch: 252, Avg_loss: 67.41307957967122\n",
      "Epoch: 253, Avg_loss: 67.07729848225911\n",
      "Epoch: 254, Avg_loss: 67.22289403279622\n",
      "Epoch: 255, Avg_loss: 66.56174723307292\n",
      "Epoch: 256, Avg_loss: 66.3277982076009\n",
      "Epoch: 257, Avg_loss: 67.34775034586589\n",
      "Epoch: 258, Avg_loss: 66.8310775756836\n",
      "Epoch: 259, Avg_loss: 67.11596298217773\n",
      "Epoch: 260, Avg_loss: 66.53430938720703\n",
      "Epoch: 261, Avg_loss: 66.63598251342773\n",
      "Epoch: 262, Avg_loss: 66.31910196940105\n",
      "Epoch: 263, Avg_loss: 66.66078694661458\n",
      "Epoch: 264, Avg_loss: 67.11957550048828\n",
      "Epoch: 265, Avg_loss: 66.97509129842122\n",
      "Epoch: 266, Avg_loss: 66.95654805501302\n",
      "Epoch: 267, Avg_loss: 67.80420176188152\n",
      "Epoch: 268, Avg_loss: 66.62722269694011\n",
      "Epoch: 269, Avg_loss: 66.20337422688802\n",
      "Epoch: 270, Avg_loss: 65.86627578735352\n",
      "Epoch: 271, Avg_loss: 66.73185221354167\n",
      "Epoch: 272, Avg_loss: 67.38288752237956\n",
      "Epoch: 273, Avg_loss: 66.19110870361328\n",
      "Epoch: 274, Avg_loss: 67.53318786621094\n",
      "Epoch: 275, Avg_loss: 66.54506047566731\n",
      "Epoch: 276, Avg_loss: 66.2403793334961\n",
      "Epoch: 277, Avg_loss: 66.55320103963216\n",
      "Epoch: 278, Avg_loss: 66.09207026163737\n",
      "Epoch: 279, Avg_loss: 66.22505442301433\n",
      "Epoch: 280, Avg_loss: 67.03019841512044\n",
      "Epoch: 281, Avg_loss: 66.23778406778972\n",
      "Epoch: 282, Avg_loss: 66.37400563557942\n",
      "Epoch: 283, Avg_loss: 65.48658243815105\n",
      "Epoch: 284, Avg_loss: 66.69900767008464\n",
      "Epoch: 285, Avg_loss: 66.49793879191081\n",
      "Epoch: 286, Avg_loss: 66.477720896403\n",
      "Epoch: 287, Avg_loss: 66.90428288777669\n",
      "Epoch: 288, Avg_loss: 66.02267328898112\n",
      "Epoch: 289, Avg_loss: 66.69854227701823\n",
      "Epoch: 290, Avg_loss: 65.96364847819011\n",
      "Epoch: 291, Avg_loss: 66.24000422159831\n",
      "Epoch: 292, Avg_loss: 67.3526611328125\n",
      "Epoch: 293, Avg_loss: 65.7755953470866\n",
      "Epoch: 294, Avg_loss: 66.2696418762207\n",
      "Epoch: 295, Avg_loss: 67.29834620157878\n",
      "Epoch: 296, Avg_loss: 67.03279876708984\n",
      "Epoch: 297, Avg_loss: 66.50360997517903\n",
      "Epoch: 298, Avg_loss: 67.02545166015625\n",
      "Epoch: 299, Avg_loss: 66.40542602539062\n",
      "Epoch: 300, Avg_loss: 67.00748952229817\n",
      "Epoch: 301, Avg_loss: 66.37503814697266\n",
      "Epoch: 302, Avg_loss: 66.31403350830078\n",
      "Epoch: 303, Avg_loss: 66.75896962483723\n",
      "Epoch: 304, Avg_loss: 65.53101221720378\n",
      "Epoch: 305, Avg_loss: 66.31645584106445\n",
      "Epoch: 306, Avg_loss: 67.24555842081706\n",
      "Epoch: 307, Avg_loss: 66.29581451416016\n",
      "Epoch: 308, Avg_loss: 65.53493118286133\n",
      "Epoch: 309, Avg_loss: 65.71659469604492\n",
      "Epoch: 310, Avg_loss: 66.85443623860677\n",
      "Epoch: 311, Avg_loss: 66.87827555338542\n",
      "Epoch: 312, Avg_loss: 65.95489756266277\n",
      "Epoch: 313, Avg_loss: 66.32488632202148\n",
      "Epoch: 314, Avg_loss: 66.77147038777669\n",
      "Epoch: 315, Avg_loss: 65.7383524576823\n",
      "Epoch: 316, Avg_loss: 66.1674919128418\n",
      "Epoch: 317, Avg_loss: 66.03279240926106\n",
      "Epoch: 318, Avg_loss: 65.95686467488606\n",
      "Epoch: 319, Avg_loss: 65.5089340209961\n",
      "Epoch: 320, Avg_loss: 66.99741872151692\n",
      "Epoch: 321, Avg_loss: 66.47834777832031\n",
      "Epoch: 322, Avg_loss: 66.6812744140625\n",
      "Epoch: 323, Avg_loss: 65.50478108723958\n",
      "Epoch: 324, Avg_loss: 65.97804514567058\n",
      "Epoch: 325, Avg_loss: 65.77677536010742\n",
      "Epoch: 326, Avg_loss: 65.65573628743489\n",
      "Epoch: 327, Avg_loss: 66.29397583007812\n",
      "Epoch: 328, Avg_loss: 66.52886454264323\n",
      "Epoch: 329, Avg_loss: 65.83914438883464\n",
      "Epoch: 330, Avg_loss: 67.04101053873698\n",
      "Epoch: 331, Avg_loss: 66.31373469034831\n",
      "Epoch: 332, Avg_loss: 65.66637420654297\n",
      "Epoch: 333, Avg_loss: 65.70319112141927\n",
      "Epoch: 334, Avg_loss: 66.57325744628906\n",
      "Epoch: 335, Avg_loss: 66.8567492167155\n",
      "Epoch: 336, Avg_loss: 65.79803975423177\n",
      "Epoch: 337, Avg_loss: 65.66963322957356\n",
      "Epoch: 338, Avg_loss: 65.28124237060547\n",
      "Epoch: 339, Avg_loss: 65.89570999145508\n",
      "Epoch: 340, Avg_loss: 66.51940663655598\n",
      "Epoch: 341, Avg_loss: 65.86152903238933\n",
      "Epoch: 342, Avg_loss: 65.74074935913086\n",
      "Epoch: 343, Avg_loss: 65.79714330037434\n",
      "Epoch: 344, Avg_loss: 65.61712900797527\n",
      "Epoch: 345, Avg_loss: 65.12933985392253\n",
      "Epoch: 346, Avg_loss: 66.90358098347981\n",
      "Epoch: 347, Avg_loss: 66.40274810791016\n",
      "Epoch: 348, Avg_loss: 66.7096455891927\n",
      "Epoch: 349, Avg_loss: 65.97627258300781\n",
      "Epoch: 350, Avg_loss: 65.26263809204102\n",
      "Epoch: 351, Avg_loss: 65.77360534667969\n",
      "Epoch: 352, Avg_loss: 65.80704116821289\n",
      "Epoch: 353, Avg_loss: 67.11897277832031\n",
      "Epoch: 354, Avg_loss: 64.81610870361328\n",
      "Epoch: 355, Avg_loss: 65.4765739440918\n",
      "Epoch: 356, Avg_loss: 65.7912000020345\n",
      "Epoch: 357, Avg_loss: 66.09488423665364\n",
      "Epoch: 358, Avg_loss: 65.48831303914388\n",
      "Epoch: 359, Avg_loss: 66.72658157348633\n",
      "Epoch: 360, Avg_loss: 66.30990727742513\n",
      "Epoch: 361, Avg_loss: 65.23153686523438\n",
      "Epoch: 362, Avg_loss: 66.6576436360677\n",
      "Epoch: 363, Avg_loss: 65.4293835957845\n",
      "Epoch: 364, Avg_loss: 66.27144368489583\n",
      "Epoch: 365, Avg_loss: 65.32662963867188\n",
      "Epoch: 366, Avg_loss: 64.91401545206706\n",
      "Epoch: 367, Avg_loss: 66.69038263956706\n",
      "Epoch: 368, Avg_loss: 65.4371566772461\n",
      "Epoch: 369, Avg_loss: 65.29210789998372\n",
      "Epoch: 370, Avg_loss: 65.8336575826009\n",
      "Epoch: 371, Avg_loss: 66.51437123616536\n",
      "Epoch: 372, Avg_loss: 65.17194493611653\n",
      "Epoch: 373, Avg_loss: 66.68193054199219\n",
      "Epoch: 374, Avg_loss: 65.12106577555339\n",
      "Epoch: 375, Avg_loss: 64.95499801635742\n",
      "Epoch: 376, Avg_loss: 65.42983754475911\n",
      "Epoch: 377, Avg_loss: 65.91336441040039\n",
      "Epoch: 378, Avg_loss: 66.3680305480957\n",
      "Epoch: 379, Avg_loss: 65.2654037475586\n",
      "Epoch: 380, Avg_loss: 65.25967788696289\n",
      "Epoch: 381, Avg_loss: 64.33855565388997\n",
      "Epoch: 382, Avg_loss: 65.58529408772786\n",
      "Epoch: 383, Avg_loss: 65.30724970499675\n",
      "Epoch: 384, Avg_loss: 64.97561136881511\n",
      "Epoch: 385, Avg_loss: 65.1992899576823\n",
      "Epoch: 386, Avg_loss: 65.31394577026367\n",
      "Epoch: 387, Avg_loss: 65.27080535888672\n",
      "Epoch: 388, Avg_loss: 64.97328821818034\n",
      "Epoch: 389, Avg_loss: 65.01791254679362\n",
      "Epoch: 390, Avg_loss: 64.39388529459636\n",
      "Epoch: 391, Avg_loss: 64.94645309448242\n",
      "Epoch: 392, Avg_loss: 65.77461115519206\n",
      "Epoch: 393, Avg_loss: 65.90708541870117\n",
      "Epoch: 394, Avg_loss: 64.85573196411133\n",
      "Epoch: 395, Avg_loss: 65.33503977457683\n",
      "Epoch: 396, Avg_loss: 64.52747599283855\n",
      "Epoch: 397, Avg_loss: 64.33347702026367\n",
      "Epoch: 398, Avg_loss: 64.99755732218425\n",
      "Epoch: 399, Avg_loss: 64.4663912455241\n",
      "Epoch: 400, Avg_loss: 64.3834622701009\n",
      "Epoch: 401, Avg_loss: 66.57784016927083\n",
      "Epoch: 402, Avg_loss: 65.18961842854817\n",
      "Epoch: 403, Avg_loss: 64.88280487060547\n",
      "Epoch: 404, Avg_loss: 65.20982487996419\n",
      "Epoch: 405, Avg_loss: 65.46314366658528\n",
      "Epoch: 406, Avg_loss: 65.03063201904297\n",
      "Epoch: 407, Avg_loss: 66.07774861653645\n",
      "Epoch: 408, Avg_loss: 64.88256200154622\n",
      "Epoch: 409, Avg_loss: 65.22774251302083\n",
      "Epoch: 410, Avg_loss: 65.22030512491862\n",
      "Epoch: 411, Avg_loss: 65.14866256713867\n",
      "Epoch: 412, Avg_loss: 65.05359268188477\n",
      "Epoch: 413, Avg_loss: 65.4470609029134\n",
      "Epoch: 414, Avg_loss: 65.58698399861653\n",
      "Epoch: 415, Avg_loss: 65.43623479207356\n",
      "Epoch: 416, Avg_loss: 64.52012634277344\n",
      "Epoch: 417, Avg_loss: 64.72798283894856\n",
      "Epoch: 418, Avg_loss: 65.58954238891602\n",
      "Epoch: 419, Avg_loss: 64.21294911702473\n",
      "Epoch: 420, Avg_loss: 65.03574117024739\n",
      "Epoch: 421, Avg_loss: 65.1310297648112\n",
      "Epoch: 422, Avg_loss: 66.25441614786784\n",
      "Epoch: 423, Avg_loss: 65.41633860270183\n",
      "Epoch: 424, Avg_loss: 65.64502461751302\n",
      "Epoch: 425, Avg_loss: 64.99417622884114\n",
      "Epoch: 426, Avg_loss: 64.48681386311848\n",
      "Epoch: 427, Avg_loss: 65.7026964823405\n",
      "Epoch: 428, Avg_loss: 64.64373524983723\n",
      "Epoch: 429, Avg_loss: 65.05305735270183\n",
      "Epoch: 430, Avg_loss: 64.72322082519531\n",
      "Epoch: 431, Avg_loss: 65.22653452555339\n",
      "Epoch: 432, Avg_loss: 65.53879292805989\n",
      "Epoch: 433, Avg_loss: 64.6899299621582\n",
      "Epoch: 434, Avg_loss: 64.73638025919597\n",
      "Epoch: 435, Avg_loss: 64.31320190429688\n",
      "Epoch: 436, Avg_loss: 64.12587356567383\n",
      "Epoch: 437, Avg_loss: 64.19596608479817\n",
      "Epoch: 438, Avg_loss: 64.06182479858398\n",
      "Epoch: 439, Avg_loss: 64.8871243794759\n",
      "Epoch: 440, Avg_loss: 64.49119440714519\n",
      "Epoch: 441, Avg_loss: 64.71954854329427\n",
      "Epoch: 442, Avg_loss: 64.24229176839192\n",
      "Epoch: 443, Avg_loss: 65.20452245076497\n",
      "Epoch: 444, Avg_loss: 64.85842259724934\n",
      "Epoch: 445, Avg_loss: 65.28791173299153\n",
      "Epoch: 446, Avg_loss: 64.86806615193684\n",
      "Epoch: 447, Avg_loss: 64.7765121459961\n",
      "Epoch: 448, Avg_loss: 64.73751068115234\n",
      "Epoch: 449, Avg_loss: 65.176575978597\n",
      "Epoch: 450, Avg_loss: 64.70302708943684\n",
      "Epoch: 451, Avg_loss: 64.1496073404948\n",
      "Epoch: 452, Avg_loss: 64.22016779581706\n",
      "Epoch: 453, Avg_loss: 64.18419774373372\n",
      "Epoch: 454, Avg_loss: 64.49649429321289\n",
      "Epoch: 455, Avg_loss: 65.72681427001953\n",
      "Epoch: 456, Avg_loss: 64.07082494099934\n",
      "Epoch: 457, Avg_loss: 64.84951782226562\n",
      "Epoch: 458, Avg_loss: 64.83263524373372\n",
      "Epoch: 459, Avg_loss: 64.72488149007161\n",
      "Epoch: 460, Avg_loss: 65.45736312866211\n",
      "Epoch: 461, Avg_loss: 64.16829299926758\n",
      "Epoch: 462, Avg_loss: 64.916566212972\n",
      "Epoch: 463, Avg_loss: 64.1160774230957\n",
      "Epoch: 464, Avg_loss: 65.61128362019856\n",
      "Epoch: 465, Avg_loss: 64.65930557250977\n",
      "Epoch: 466, Avg_loss: 64.63258870442708\n",
      "Epoch: 467, Avg_loss: 64.29691823323567\n",
      "Epoch: 468, Avg_loss: 64.2664794921875\n",
      "Epoch: 469, Avg_loss: 63.61734135945638\n",
      "Epoch: 470, Avg_loss: 66.02501169840495\n",
      "Epoch: 471, Avg_loss: 64.56748708089192\n",
      "Epoch: 472, Avg_loss: 63.279398600260414\n",
      "Epoch: 473, Avg_loss: 65.09571711222331\n",
      "Epoch: 474, Avg_loss: 63.402886708577476\n",
      "Epoch: 475, Avg_loss: 64.27657063802083\n",
      "Epoch: 476, Avg_loss: 63.56726837158203\n",
      "Epoch: 477, Avg_loss: 64.22991307576497\n",
      "Epoch: 478, Avg_loss: 64.36067708333333\n",
      "Epoch: 479, Avg_loss: 64.47058232625325\n",
      "Epoch: 480, Avg_loss: 64.41893005371094\n",
      "Epoch: 481, Avg_loss: 64.401673634847\n",
      "Epoch: 482, Avg_loss: 64.34402084350586\n",
      "Epoch: 483, Avg_loss: 64.32904307047527\n",
      "Epoch: 484, Avg_loss: 65.17876434326172\n",
      "Epoch: 485, Avg_loss: 64.09853871663411\n",
      "Epoch: 486, Avg_loss: 64.31909561157227\n",
      "Epoch: 487, Avg_loss: 63.53850301106771\n",
      "Epoch: 488, Avg_loss: 64.04109700520833\n",
      "Epoch: 489, Avg_loss: 65.24745814005534\n",
      "Epoch: 490, Avg_loss: 64.02548217773438\n",
      "Epoch: 491, Avg_loss: 64.15678787231445\n",
      "Epoch: 492, Avg_loss: 64.14919789632161\n",
      "Epoch: 493, Avg_loss: 64.98438262939453\n",
      "Epoch: 494, Avg_loss: 64.90990447998047\n",
      "Epoch: 495, Avg_loss: 63.712869008382164\n",
      "Epoch: 496, Avg_loss: 63.73952992757162\n",
      "Epoch: 497, Avg_loss: 65.19672775268555\n",
      "Epoch: 498, Avg_loss: 63.64421081542969\n",
      "Epoch: 499, Avg_loss: 64.37304814656575\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from training import NeuralProcessTrainer\n",
    "\n",
    "batch_size = 1\n",
    "num_context = 17\n",
    "num_target = 1\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(neuralprocess.parameters(), lr=3e-4)\n",
    "np_trainer = NeuralProcessTrainer(device, neuralprocess, optimizer,\n",
    "                                  num_context_range=(num_context, num_context),\n",
    "                                  num_extra_target_range=(num_target, num_target), \n",
    "                                  print_freq=200)\n",
    "\n",
    "neuralprocess.training = True\n",
    "np_trainer.train(data_loader, 500)\n",
    "#save model\n",
    "torch.save(neuralprocess.state_dict(), r'D:\\PycharmProjects\\ANP\\neural-processes-oxford\\trained_models\\pretrained.ckpt')\n",
    "plt.plot(range(len(np_trainer.epoch_loss_history)),np_trainer.epoch_loss_history)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### predict without context\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[[51.],\n",
      "         [18.],\n",
      "         [47.],\n",
      "         [25.],\n",
      "         [20.],\n",
      "         [18.],\n",
      "         [25.],\n",
      "         [35.],\n",
      "         [58.],\n",
      "         [20.],\n",
      "         [23.],\n",
      "         [35.],\n",
      "         [38.],\n",
      "         [49.],\n",
      "         [58.],\n",
      "         [60.],\n",
      "         [49.],\n",
      "         [61.]]])\n",
      "tensor([[[35.3414],\n",
      "         [14.7679],\n",
      "         [44.8555],\n",
      "         [13.2495],\n",
      "         [17.1841],\n",
      "         [15.1566],\n",
      "         [17.0036],\n",
      "         [25.5948],\n",
      "         [48.5223],\n",
      "         [17.3840],\n",
      "         [17.5946],\n",
      "         [42.2661],\n",
      "         [21.3527],\n",
      "         [22.4635],\n",
      "         [55.8299],\n",
      "         [40.9011],\n",
      "         [31.9981],\n",
      "         [34.2574]]])\n",
      "tensor([[[38.0105],\n",
      "         [17.4563],\n",
      "         [47.5233],\n",
      "         [15.9462],\n",
      "         [19.8717],\n",
      "         [17.8341],\n",
      "         [19.6939],\n",
      "         [28.3013],\n",
      "         [51.1490],\n",
      "         [20.0718],\n",
      "         [20.2870],\n",
      "         [44.9462],\n",
      "         [24.0530],\n",
      "         [25.1481],\n",
      "         [58.3870],\n",
      "         [43.4635],\n",
      "         [34.6911],\n",
      "         [36.8871]]])\n",
      "tensor([[[38.3126],\n",
      "         [17.7739],\n",
      "         [47.8316],\n",
      "         [16.2636],\n",
      "         [20.1841],\n",
      "         [18.1515],\n",
      "         [20.0114],\n",
      "         [28.6154],\n",
      "         [51.4414],\n",
      "         [20.3854],\n",
      "         [20.5973],\n",
      "         [45.2734],\n",
      "         [24.3812],\n",
      "         [25.4657],\n",
      "         [58.6617],\n",
      "         [43.7379],\n",
      "         [35.0141],\n",
      "         [37.1865]]])\n",
      "tensor([[[34.8772],\n",
      "         [14.3050],\n",
      "         [44.3823],\n",
      "         [12.7933],\n",
      "         [16.7170],\n",
      "         [14.6887],\n",
      "         [16.5465],\n",
      "         [25.1519],\n",
      "         [48.0502],\n",
      "         [16.9183],\n",
      "         [17.1317],\n",
      "         [41.8018],\n",
      "         [20.8986],\n",
      "         [22.0093],\n",
      "         [55.3498],\n",
      "         [40.4194],\n",
      "         [31.5356],\n",
      "         [33.7960]]])\n",
      "tensor([[[37.7878],\n",
      "         [17.2495],\n",
      "         [47.3047],\n",
      "         [15.7392],\n",
      "         [19.6605],\n",
      "         [17.6269],\n",
      "         [19.4869],\n",
      "         [28.0946],\n",
      "         [50.9333],\n",
      "         [19.8609],\n",
      "         [20.0758],\n",
      "         [44.7380],\n",
      "         [23.8540],\n",
      "         [24.9504],\n",
      "         [58.1685],\n",
      "         [43.2431],\n",
      "         [34.4803],\n",
      "         [36.6749]]])\n",
      "tensor([[[34.9373],\n",
      "         [14.3382],\n",
      "         [44.4381],\n",
      "         [12.8264],\n",
      "         [16.7545],\n",
      "         [14.7204],\n",
      "         [16.5790],\n",
      "         [25.1851],\n",
      "         [48.1269],\n",
      "         [16.9547],\n",
      "         [17.1730],\n",
      "         [41.8528],\n",
      "         [20.9268],\n",
      "         [22.0308],\n",
      "         [55.4485],\n",
      "         [40.5130],\n",
      "         [31.5900],\n",
      "         [33.8601]]])\n",
      "tensor([[[35.9441],\n",
      "         [15.3808],\n",
      "         [45.4610],\n",
      "         [13.8706],\n",
      "         [17.7911],\n",
      "         [15.7638],\n",
      "         [17.6183],\n",
      "         [26.2381],\n",
      "         [49.1066],\n",
      "         [17.9923],\n",
      "         [18.2049],\n",
      "         [42.9063],\n",
      "         [21.9823],\n",
      "         [23.0903],\n",
      "         [56.3696],\n",
      "         [41.4477],\n",
      "         [32.6405],\n",
      "         [34.8508]]])\n",
      "tensor([[[38.4357],\n",
      "         [17.8882],\n",
      "         [47.9527],\n",
      "         [16.3796],\n",
      "         [20.3040],\n",
      "         [18.2660],\n",
      "         [20.1259],\n",
      "         [28.7340],\n",
      "         [51.5590],\n",
      "         [20.5037],\n",
      "         [20.7189],\n",
      "         [45.3883],\n",
      "         [24.4922],\n",
      "         [25.5775],\n",
      "         [58.7793],\n",
      "         [43.8548],\n",
      "         [35.1276],\n",
      "         [37.2964]]])\n",
      "tensor([[[35.9240],\n",
      "         [15.3916],\n",
      "         [45.4408],\n",
      "         [13.8789],\n",
      "         [17.8022],\n",
      "         [15.7703],\n",
      "         [17.6310],\n",
      "         [26.2382],\n",
      "         [49.0858],\n",
      "         [18.0035],\n",
      "         [18.2177],\n",
      "         [42.8902],\n",
      "         [21.9905],\n",
      "         [23.0870],\n",
      "         [56.3403],\n",
      "         [41.4161],\n",
      "         [32.6277],\n",
      "         [34.8265]]])\n",
      "tensor([[[33.4966],\n",
      "         [12.8791],\n",
      "         [42.9937],\n",
      "         [11.3556],\n",
      "         [15.2851],\n",
      "         [13.3011],\n",
      "         [15.1160],\n",
      "         [23.7126],\n",
      "         [46.7040],\n",
      "         [15.4865],\n",
      "         [15.7006],\n",
      "         [40.3837],\n",
      "         [19.4531],\n",
      "         [20.5691],\n",
      "         [54.0376],\n",
      "         [39.1090],\n",
      "         [30.1109],\n",
      "         [32.4287]]])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for batch in data_loader:\n",
    "    break\n",
    "x_target, y_target = batch\n",
    "print(y_target)\n",
    "# print(x_target)\n",
    "# print(x_target.size())\n",
    "for i in range(10):\n",
    "    z_sample = torch.randn((1, z_dim))\n",
    "    mu, _ = neuralprocess.xz_to_y(x_target, z_sample)\n",
    "    print(mu.detach())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### predict with context\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "prediceted value: \n",
      "tensor([[[72.5631],\n",
      "         [40.2197],\n",
      "         [25.6759],\n",
      "         [22.1763],\n",
      "         [41.0048],\n",
      "         [27.4786],\n",
      "         [23.6717],\n",
      "         [31.0153],\n",
      "         [36.3050],\n",
      "         [19.4988],\n",
      "         [24.4813],\n",
      "         [34.2493],\n",
      "         [21.5662],\n",
      "         [43.9641],\n",
      "         [50.2672],\n",
      "         [45.7708],\n",
      "         [18.1740],\n",
      "         [26.2333]]])\n",
      "target value:\n",
      "tensor([[[63.],\n",
      "         [62.],\n",
      "         [48.],\n",
      "         [28.],\n",
      "         [53.],\n",
      "         [30.],\n",
      "         [26.],\n",
      "         [53.],\n",
      "         [30.],\n",
      "         [19.],\n",
      "         [21.],\n",
      "         [30.],\n",
      "         [30.],\n",
      "         [37.],\n",
      "         [51.],\n",
      "         [53.],\n",
      "         [28.],\n",
      "         [40.]]])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from utils import context_target_split\n",
    "\n",
    "for batch in data_loader:\n",
    "    break\n",
    "\n",
    "# Use batch to create random set of context points\n",
    "x, y = batch\n",
    "x_context, y_context, _, _ = context_target_split(x[0:1], y[0:1], \n",
    "                                                  num_context, \n",
    "                                                  num_target)\n",
    "neuralprocess.training = False\n",
    "from datasets import FaceFeatureTestData\n",
    "\n",
    "testDataset = FaceFeatureTestData()\n",
    "testData_loader = DataLoader(testDataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for x_target, y_target in testData_loader:\n",
    "    avg_mu = 0\n",
    "    for i in range(10):\n",
    "        p_y_pred = neuralprocess(x_context, y_context, x_target)\n",
    "        # Extract mean of distribution\n",
    "        mu = p_y_pred.loc.detach()\n",
    "        avg_mu += mu\n",
    "    print('prediceted value: ')\n",
    "    print(avg_mu / 10)\n",
    "    print('target value:')\n",
    "    print(y_target)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "prediceted value: \n",
      "tensor([[[72.5237],\n",
      "         [40.1795],\n",
      "         [25.6178],\n",
      "         [22.1195],\n",
      "         [40.9648],\n",
      "         [27.4218],\n",
      "         [23.6144],\n",
      "         [30.9737],\n",
      "         [36.2488],\n",
      "         [19.4426],\n",
      "         [24.4230],\n",
      "         [34.1927],\n",
      "         [21.5088],\n",
      "         [43.9165],\n",
      "         [50.2277],\n",
      "         [45.7309],\n",
      "         [18.1189],\n",
      "         [26.1737]]])\n",
      "target value:\n",
      "tensor([[[63.],\n",
      "         [62.],\n",
      "         [48.],\n",
      "         [28.],\n",
      "         [53.],\n",
      "         [30.],\n",
      "         [26.],\n",
      "         [53.],\n",
      "         [30.],\n",
      "         [19.],\n",
      "         [21.],\n",
      "         [30.],\n",
      "         [30.],\n",
      "         [37.],\n",
      "         [51.],\n",
      "         [53.],\n",
      "         [28.],\n",
      "         [40.]]])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from utils import context_target_split\n",
    "\n",
    "for batch in data_loader:\n",
    "    break\n",
    "\n",
    "# Use batch to create random set of context points\n",
    "x, y = batch\n",
    "x_context, y_context, _, _ = context_target_split(x[0:1], y[0:1], \n",
    "                                                  num_context, \n",
    "                                                  num_target)\n",
    "\n",
    "#load model\n",
    "testNeuralprocess = NeuralProcess(x_dim, y_dim, r_dim, z_dim, h_dim)\n",
    "testNeuralprocess.load_state_dict(torch.load(r'D:\\PycharmProjects\\ANP\\neural-processes-oxford\\trained_models\\pretrained.ckpt'))\n",
    "testNeuralprocess.training = False\n",
    "\n",
    "from datasets import FaceFeatureTestData\n",
    "\n",
    "testDataset = FaceFeatureTestData()\n",
    "testData_loader = DataLoader(testDataset, batch_size=batch_size, shuffle=True)\n",
    "for x_target, y_target in testData_loader:\n",
    "    avg_mu = 0\n",
    "    for i in range(10):\n",
    "        p_y_pred = testNeuralprocess(x_context, y_context, x_target)\n",
    "        # Extract mean of distribution\n",
    "        mu = p_y_pred.loc.detach()\n",
    "        avg_mu += mu\n",
    "    print('prediceted value: ')\n",
    "    print(avg_mu / 10)\n",
    "    print('target value:')\n",
    "    print(y_target)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}